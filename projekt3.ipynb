{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 44,
            "id": "initial_id",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-04-09T20:30:00.543627Z",
                    "start_time": "2025-04-09T20:29:56.320675Z"
                },
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torchvision.transforms as transforms\n",
                "from torchvision.datasets import ImageFolder\n",
                "from torch.utils.data import DataLoader\n",
                "import time\n",
                "import matplotlib.pyplot as plt\n",
                "from torchinfo import summary\n",
                "import json\n",
                "import os\n",
                "import shutil\n",
                "from torch.utils.data import random_split\n",
                "import csv\n",
                "from PIL import Image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "id": "d8773f97620cd5b1",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-04-09T20:31:03.356617Z",
                    "start_time": "2025-04-09T20:31:02.979838Z"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Liczba klas: 50\n",
                        "Nazwy klas: ['acoustic', 'antenna', 'bacteria', 'battery', 'bean', 'beetle', 'bicycle', 'birch', 'bird', 'bomb', 'bread', 'bridge', 'camera', 'carbon', 'cat', 'corn', 'crab', 'crocodilian', 'echinoderm', 'egg', 'elephant', 'fish', 'flower', 'frog', 'fungus', 'gauge', 'hammer', 'icecream', 'kangaroo', 'memorial', 'monkey', 'motor', 'nest', 'palm', 'pizza', 'pot', 'printer', 'saw', 'snake', 'spice', 'spider', 'spoon', 'squash', 'swine', 'tea', 'tomato', 'towel', 'truck', 'turtle', 'worm']\n",
                        "Liczba obrazów treningowych: 80970\n",
                        "Liczba obrazów walidacyjnych: 7041\n"
                    ]
                }
            ],
            "source": [
                "train_transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomRotation(10),\n",
                "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "test_transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "full_dataset = ImageFolder(\"dane/train/\", transform=train_transform)\n",
                "\n",
                "dataset_size = len(full_dataset)\n",
                "train_size = int(dataset_size * 0.92)\n",
                "val_size = dataset_size - train_size\n",
                "trainset, valset = random_split(full_dataset, [train_size, val_size])\n",
                "\n",
                "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=4)\n",
                "valloader = DataLoader(valset, batch_size=32, shuffle=False, num_workers=4)\n",
                "\n",
                "print(f\"Liczba klas: {len(full_dataset.classes)}\")\n",
                "print(f\"Nazwy klas: {full_dataset.classes}\")\n",
                "print(f\"Liczba obrazów treningowych: {len(trainset)}\")\n",
                "print(f\"Liczba obrazów walidacyjnych: {len(valset)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a2688c7e8fb10b0f",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-04-09T20:31:37.200085Z",
                    "start_time": "2025-04-09T20:31:37.189466Z"
                }
            },
            "outputs": [],
            "source": [
                "class CNNModel(nn.Module):\n",
                "    def __init__(self, num_classes):\n",
                "        super(CNNModel, self).__init__()\n",
                "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
                "        self.bn1 = nn.BatchNorm2d(32)\n",
                "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "\n",
                "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
                "        self.bn2 = nn.BatchNorm2d(64)\n",
                "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "\n",
                "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
                "        self.bn3 = nn.BatchNorm2d(128)\n",
                "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "\n",
                "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
                "        self.bn4 = nn.BatchNorm2d(256)\n",
                "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "\n",
                "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
                "        self.bn5 = nn.BatchNorm2d(512)\n",
                "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "\n",
                "        self.fc1 = nn.Linear(512 * 7 * 7, 1024)\n",
                "        self.dropout = nn.Dropout(0.5)\n",
                "        self.fc2 = nn.Linear(1024, num_classes)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
                "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
                "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
                "        x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
                "        x = self.pool5(F.relu(self.bn5(self.conv5(x))))\n",
                "\n",
                "        x = x.view(-1, 512 * 7 * 7)\n",
                "\n",
                "        x = F.relu(self.fc1(x))\n",
                "        x = self.dropout(x)\n",
                "        x = self.fc2(x)\n",
                "\n",
                "        return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "id": "d9a39403",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Używane urządzenie: cuda:0\n"
                    ]
                }
            ],
            "source": [
                "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Używane urządzenie: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "id": "d63d6e14",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "No existing model found. Initializing new model.\n"
                    ]
                }
            ],
            "source": [
                "num_classes = len(full_dataset.classes)\n",
                "model = CNNModel(num_classes).to(device)\n",
                "\n",
                "model_path = 'model.pth'\n",
                "if os.path.exists(model_path):\n",
                "    print(f\"Loading existing model from {model_path}\")\n",
                "    model.load_state_dict(torch.load(model_path))\n",
                "else:\n",
                "    print(\"No existing model found. Initializing new model.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "id": "28a2844a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "==========================================================================================\n",
                        "Layer (type:depth-idx)                   Output Shape              Param #\n",
                        "==========================================================================================\n",
                        "CNNModel                                 [1, 50]                   --\n",
                        "├─Conv2d: 1-1                            [1, 32, 224, 224]         896\n",
                        "│    └─weight                                                      ├─864\n",
                        "│    └─bias                                                        └─32\n",
                        "├─BatchNorm2d: 1-2                       [1, 32, 224, 224]         64\n",
                        "│    └─weight                                                      ├─32\n",
                        "│    └─bias                                                        └─32\n",
                        "├─MaxPool2d: 1-3                         [1, 32, 74, 74]           --\n",
                        "├─Conv2d: 1-4                            [1, 64, 74, 74]           18,496\n",
                        "│    └─weight                                                      ├─18,432\n",
                        "│    └─bias                                                        └─64\n",
                        "├─BatchNorm2d: 1-5                       [1, 64, 74, 74]           128\n",
                        "│    └─weight                                                      ├─64\n",
                        "│    └─bias                                                        └─64\n",
                        "├─MaxPool2d: 1-6                         [1, 64, 24, 24]           --\n",
                        "├─Conv2d: 1-7                            [1, 128, 24, 24]          73,856\n",
                        "│    └─weight                                                      ├─73,728\n",
                        "│    └─bias                                                        └─128\n",
                        "├─BatchNorm2d: 1-8                       [1, 128, 24, 24]          256\n",
                        "│    └─weight                                                      ├─128\n",
                        "│    └─bias                                                        └─128\n",
                        "├─MaxPool2d: 1-9                         [1, 128, 8, 8]            --\n",
                        "├─Linear: 1-10                           [1, 512]                  4,194,816\n",
                        "│    └─weight                                                      ├─4,194,304\n",
                        "│    └─bias                                                        └─512\n",
                        "├─Dropout: 1-11                          [1, 512]                  --\n",
                        "├─Linear: 1-12                           [1, 50]                   25,650\n",
                        "│    └─weight                                                      ├─25,600\n",
                        "│    └─bias                                                        └─50\n",
                        "==========================================================================================\n",
                        "Total params: 4,314,162\n",
                        "Trainable params: 4,314,162\n",
                        "Non-trainable params: 0\n",
                        "Total mult-adds (Units.MEGABYTES): 193.00\n",
                        "==========================================================================================\n",
                        "Input size (MB): 0.60\n",
                        "Forward/backward pass size (MB): 32.48\n",
                        "Params size (MB): 17.26\n",
                        "Estimated Total Size (MB): 50.34\n",
                        "==========================================================================================\n"
                    ]
                }
            ],
            "source": [
                "summary(model, input_size=(1, 3, 224, 224), verbose=2)\n",
                "\n",
                "model_stats = summary(\n",
                "    model,\n",
                "    input_size=(1, 3, 224, 224),\n",
                "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"],\n",
                "    col_width=20,\n",
                "    row_settings=[\"var_names\"]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "id": "603acdcef363b06c",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-04-09T20:36:59.388485Z",
                    "start_time": "2025-04-09T20:31:41.798386Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/jszubzda/proj3-SSNE/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/10 | Batch 2500/2531 | Loss: 3.3359 | Acc: 12.62% | Epoch time: 118.4s | Total time: 118.4s\n",
                        "Epoch 1 - Train Loss: 3.3356, Train Acc: 12.62%, Val Loss: 3.0035, Val Acc: 20.98%, LR: 0.001000\n",
                        "Epoch 2/10 | Batch 2500/2531 | Loss: 3.1453 | Acc: 16.44% | Epoch time: 123.6s | Total time: 250.8s\n",
                        "Epoch 2 - Train Loss: 3.1452, Train Acc: 16.43%, Val Loss: 2.8189, Val Acc: 24.41%, LR: 0.001000\n",
                        "Epoch 3/10 | Batch 2500/2531 | Loss: 3.0018 | Acc: 19.82% | Epoch time: 125.8s | Total time: 388.1s\n",
                        "Epoch 3 - Train Loss: 3.0009, Train Acc: 19.82%, Val Loss: 2.6527, Val Acc: 29.71%, LR: 0.001000\n",
                        "Epoch 4/10 | Batch 2500/2531 | Loss: 2.8724 | Acc: 22.49% | Epoch time: 121.5s | Total time: 521.7s\n",
                        "Epoch 4 - Train Loss: 2.8733, Train Acc: 22.49%, Val Loss: 2.5282, Val Acc: 32.38%, LR: 0.001000\n",
                        "Epoch 5/10 | Batch 2500/2531 | Loss: 2.7883 | Acc: 24.90% | Epoch time: 121.9s | Total time: 655.5s\n",
                        "Epoch 5 - Train Loss: 2.7882, Train Acc: 24.91%, Val Loss: 2.4448, Val Acc: 34.04%, LR: 0.001000\n",
                        "Epoch 6/10 | Batch 2500/2531 | Loss: 2.7168 | Acc: 26.58% | Epoch time: 120.5s | Total time: 788.0s\n",
                        "Epoch 6 - Train Loss: 2.7165, Train Acc: 26.59%, Val Loss: 2.3777, Val Acc: 36.88%, LR: 0.001000\n",
                        "Epoch 7/10 | Batch 2500/2531 | Loss: 2.6524 | Acc: 28.10% | Epoch time: 115.0s | Total time: 912.4s\n",
                        "Epoch 7 - Train Loss: 2.6529, Train Acc: 28.09%, Val Loss: 2.3032, Val Acc: 38.13%, LR: 0.001000\n",
                        "Epoch 8/10 | Batch 2500/2531 | Loss: 2.5992 | Acc: 29.51% | Epoch time: 117.6s | Total time: 1041.8s\n",
                        "Epoch 8 - Train Loss: 2.5994, Train Acc: 29.51%, Val Loss: 2.2458, Val Acc: 39.74%, LR: 0.001000\n",
                        "Epoch 9/10 | Batch 2500/2531 | Loss: 2.5466 | Acc: 30.55% | Epoch time: 118.4s | Total time: 1170.1s\n",
                        "Epoch 9 - Train Loss: 2.5462, Train Acc: 30.54%, Val Loss: 2.2179, Val Acc: 40.34%, LR: 0.001000\n",
                        "Epoch 10/10 | Batch 2500/2531 | Loss: 2.4963 | Acc: 31.76% | Epoch time: 115.2s | Total time: 1294.6s\n",
                        "Epoch 10 - Train Loss: 2.4969, Train Acc: 31.75%, Val Loss: 2.1872, Val Acc: 41.37%, LR: 0.001000\n",
                        "Training complete. Final model saved as main.pth\n"
                    ]
                }
            ],
            "source": [
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
                "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, verbose=True)\n",
                "\n",
                "\n",
                "def train_model(model, trainloader, valloader, criterion, optimizer, scheduler, num_epochs=10):\n",
                "    os.makedirs(\"models\", exist_ok=True)\n",
                "\n",
                "    model.train()\n",
                "    start_time = time.time()\n",
                "\n",
                "    training_losses = []\n",
                "    training_accuracies = []\n",
                "    val_losses = []\n",
                "    val_accuracies = []\n",
                "\n",
                "    for epoch in range(num_epochs):\n",
                "        model.train()\n",
                "        running_loss = 0.0\n",
                "        correct = 0\n",
                "        total = 0\n",
                "\n",
                "        batch_count = len(trainloader)\n",
                "        epoch_start = time.time()\n",
                "\n",
                "        for i, data in enumerate(trainloader, 0):\n",
                "            inputs, labels = data[0].to(device), data[1].to(device)\n",
                "\n",
                "            optimizer.zero_grad()\n",
                "\n",
                "            outputs = model(inputs)\n",
                "            loss = criterion(outputs, labels)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "\n",
                "            running_loss += loss.item()\n",
                "            _, predicted = torch.max(outputs.data, 1)\n",
                "            total += labels.size(0)\n",
                "            correct += (predicted == labels).sum().item()\n",
                "\n",
                "            accuracy = 100 * correct / total\n",
                "            elapsed = time.time() - start_time\n",
                "            epoch_elapsed = time.time() - epoch_start\n",
                "            if (i + 1) % 50 == 0:\n",
                "                print(f\"\\rEpoch {epoch+1}/{num_epochs} | Batch {i+1}/{batch_count} | Loss: {running_loss/(i+1):.4f} | Acc: {accuracy:.2f}% | Epoch time: {epoch_elapsed:.1f}s | Total time: {elapsed:.1f}s\", end=\"\")\n",
                "\n",
                "        train_loss = running_loss / len(trainloader)\n",
                "        train_acc = 100 * correct / total\n",
                "        training_losses.append(train_loss)\n",
                "        training_accuracies.append(train_acc)\n",
                "\n",
                "        model.eval()\n",
                "        val_loss = 0.0\n",
                "        val_correct = 0\n",
                "        val_total = 0\n",
                "\n",
                "        with torch.no_grad():\n",
                "            for data in valloader:\n",
                "                inputs, labels = data[0].to(device), data[1].to(device)\n",
                "                outputs = model(inputs)\n",
                "                loss = criterion(outputs, labels)\n",
                "\n",
                "                val_loss += loss.item()\n",
                "                _, predicted = torch.max(outputs.data, 1)\n",
                "                val_total += labels.size(0)\n",
                "                val_correct += (predicted == labels).sum().item()\n",
                "\n",
                "        val_loss = val_loss / len(valloader)\n",
                "        val_acc = 100 * val_correct / val_total\n",
                "        val_losses.append(val_loss)\n",
                "        val_accuracies.append(val_acc)\n",
                "\n",
                "        scheduler.step(val_loss)\n",
                "\n",
                "        lr = optimizer.param_groups[0]['lr']\n",
                "        print(f\"\\nEpoch {epoch+1} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, LR: {lr:.6f}\")\n",
                "\n",
                "        if epoch > 0:\n",
                "            prev_backup = f\"backup_model_epoch_{epoch}.pth\"\n",
                "            if os.path.exists(prev_backup):\n",
                "                os.remove(prev_backup)\n",
                "\n",
                "        backup_path = f\"backup_model_epoch_{epoch+1}.pth\"\n",
                "        torch.save(model.state_dict(), backup_path)\n",
                "\n",
                "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
                "            model_dir = f\"models/model_{epoch+1}\"\n",
                "            os.makedirs(model_dir, exist_ok=True)\n",
                "\n",
                "            model_path = f\"{model_dir}/model.pth\"\n",
                "            torch.save(model.state_dict(), model_path)\n",
                "\n",
                "            plt.figure(figsize=(12, 5))\n",
                "\n",
                "            plt.subplot(1, 2, 1)\n",
                "            plt.plot(training_losses, 'b-', label='Training')\n",
                "            plt.plot(val_losses, 'r-', label='Validation')\n",
                "            plt.title('Loss During Training')\n",
                "            plt.xlabel('Epochs')\n",
                "            plt.ylabel('Loss')\n",
                "            plt.legend()\n",
                "\n",
                "            plt.subplot(1, 2, 2)\n",
                "            plt.plot(training_accuracies, 'b-', label='Training')\n",
                "            plt.plot(val_accuracies, 'r-', label='Validation')\n",
                "            plt.title('Accuracy During Training')\n",
                "            plt.xlabel('Epochs')\n",
                "            plt.ylabel('Accuracy (%)')\n",
                "            plt.legend()\n",
                "\n",
                "            plt.tight_layout()\n",
                "            plt.savefig(f\"{model_dir}/training_history.png\")\n",
                "            plt.close()\n",
                "\n",
                "            training_data = {\n",
                "                'epoch': epoch + 1,\n",
                "                'train_losses': training_losses,\n",
                "                'train_accs': training_accuracies,\n",
                "                'val_losses': val_losses,\n",
                "                'val_accs': val_accuracies,\n",
                "                'final_lr': lr\n",
                "            }\n",
                "\n",
                "            with open(f\"{model_dir}/training_data.json\", 'w') as f:\n",
                "                json.dump(training_data, f)\n",
                "\n",
                "    torch.save(model.state_dict(), 'main.pth')\n",
                "    print(\"Training complete. Final model saved as main.pth\")\n",
                "\n",
                "    return {\n",
                "        'train_losses': training_losses,\n",
                "        'train_accs': training_accuracies,\n",
                "        'val_losses': val_losses,\n",
                "        'val_accs': val_accuracies\n",
                "    }\n",
                "\n",
                "history = train_model(model, trainloader, valloader, criterion, optimizer, scheduler, num_epochs=10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ecd6869e",
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(history['train_losses'], 'b-', label='Training')\n",
                "plt.plot(history['val_losses'], 'r-', label='Validation')\n",
                "plt.title('Loss During Training')\n",
                "plt.xlabel('Epochs')\n",
                "plt.ylabel('Loss')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(history['train_accs'], 'b-', label='Training')\n",
                "plt.plot(history['val_accs'], 'r-', label='Validation')\n",
                "plt.title('Accuracy During Training')\n",
                "plt.xlabel('Epochs')\n",
                "plt.ylabel('Accuracy (%)')\n",
                "plt.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# generate the test set predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.load_state_dict(torch.load('main.pth'))\n",
                "model.eval()\n",
                "\n",
                "test_dir = \"dane/test/\"\n",
                "test_files = [f for f in os.listdir(test_dir)\n",
                "              if f.lower().endswith(('.jpeg', '.jpg', '.png', '.JPEG'))]\n",
                "\n",
                "print(f\"Found {len(test_files)} test images\")\n",
                "print(f\"Classes in training set: {full_dataset.classes}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predictions = []\n",
                "filenames = []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for file in test_files:\n",
                "        img_path = os.path.join(test_dir, file)\n",
                "        image = Image.open(img_path).convert('RGB')\n",
                "        image_tensor = test_transform(image).unsqueeze(0).to(device)\n",
                "\n",
                "        output = model(image_tensor)\n",
                "        _, pred = torch.max(output, 1)\n",
                "\n",
                "        base_name = os.path.splitext(file)[0]\n",
                "        filename_jpeg = f\"{base_name}.JPEG\"\n",
                "\n",
                "        predictions.append(pred.item())\n",
                "        filenames.append(filename_jpeg)\n",
                "\n",
                "        if len(predictions) % 100 == 0:\n",
                "            print(f\"Processed {len(predictions)}/{len(test_files)} images\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open('pred.csv', 'w', newline='') as csvfile:\n",
                "    writer = csv.writer(csvfile)\n",
                "    for filename, pred in zip(filenames, predictions):\n",
                "        writer.writerow([filename, pred])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
